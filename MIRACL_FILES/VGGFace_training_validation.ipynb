{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#../input/fork-of-dlib-extraction/F01106mouth6.jpg\n#../input/keypoints-file/keypoints_df.csv","metadata":{"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport dlib\nimport cv2\nfrom matplotlib import pyplot as plt\nfrom PIL import Image","metadata":{"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/path-df-new/keypoints_df (1).csv')","metadata":{"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"       person_id  word_id  image_id          mouth_path\n0              1        1         1     F0101mouth1.jpg\n1              1        1         2     F0101mouth2.jpg\n2              1        1         3     F0101mouth3.jpg\n3              1        1         4     F0101mouth4.jpg\n4              1        1         5     F0101mouth5.jpg\n...          ...      ...       ...                 ...\n14994         16       10         6   F016010mouth6.jpg\n14995         16       10         7   F016010mouth7.jpg\n14996         16       10         8   F016010mouth8.jpg\n14997         16       10         9   F016010mouth9.jpg\n14998         16       10        10  F016010mouth10.jpg\n\n[14999 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>person_id</th>\n      <th>word_id</th>\n      <th>image_id</th>\n      <th>mouth_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>F0101mouth1.jpg</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>F0101mouth2.jpg</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>F0101mouth3.jpg</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1</td>\n      <td>4</td>\n      <td>F0101mouth4.jpg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>1</td>\n      <td>5</td>\n      <td>F0101mouth5.jpg</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>14994</th>\n      <td>16</td>\n      <td>10</td>\n      <td>6</td>\n      <td>F016010mouth6.jpg</td>\n    </tr>\n    <tr>\n      <th>14995</th>\n      <td>16</td>\n      <td>10</td>\n      <td>7</td>\n      <td>F016010mouth7.jpg</td>\n    </tr>\n    <tr>\n      <th>14996</th>\n      <td>16</td>\n      <td>10</td>\n      <td>8</td>\n      <td>F016010mouth8.jpg</td>\n    </tr>\n    <tr>\n      <th>14997</th>\n      <td>16</td>\n      <td>10</td>\n      <td>9</td>\n      <td>F016010mouth9.jpg</td>\n    </tr>\n    <tr>\n      <th>14998</th>\n      <td>16</td>\n      <td>10</td>\n      <td>10</td>\n      <td>F016010mouth10.jpg</td>\n    </tr>\n  </tbody>\n</table>\n<p>14999 rows × 4 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"input_path = '../input/fork-of-fork-of-dlib-extraction/'","metadata":{"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"a=[]\nb=[]","metadata":{"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"w1 = [[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]]\nw2 = [[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]]\nw3 = [[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]]\nw4 = [[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]]\nw5 = [[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]]\nw6 = [[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]]\nw7 = [[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]]\nw8 = [[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]]\nw9 = [[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]]\nw10= [[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]]\n\nm = 0\nfor p in df['person_id'].unique():\n    first_words_path = input_path + 'F0' + str(p)\n        \n    for i in df['word_id'].unique():\n        single_word_path = first_words_path + '0' + str(i)\n        \n        if (i==1):  \n            for k in df['image_id'].unique():\n                \n                image_path = single_word_path + 'mouth'+ str(k) + '.jpg'\n                image = cv2.imread(image_path)\n                w1[m].append(image)\n        if (i==2):  \n            for k in df['image_id'].unique():\n                \n                image_path = single_word_path + 'mouth'+ str(k) + '.jpg'\n                image = cv2.imread(image_path)\n                w2[m].append(image)\n        if (i==3):            \n            for k in df['image_id'].unique():\n\n                image_path = single_word_path + 'mouth'+ str(k) + '.jpg'\n                image = cv2.imread(image_path)\n                w3[m].append(image)  \n                    \n        if (i==4):            \n            for k in df['image_id'].unique():\n                \n                image_path = single_word_path + 'mouth'+ str(k) + '.jpg'\n                image = cv2.imread(image_path)\n                w4[m].append(image)\n                    \n        if (i==5):            \n            for k in df['image_id'].unique():\n                \n                image_path = single_word_path + 'mouth'+ str(k) + '.jpg'\n                image = cv2.imread(image_path)\n                w5[m].append(image)\n                    \n        if (i==6):            \n            for k in df['image_id'].unique():\n                \n                image_path = single_word_path + 'mouth'+ str(k) + '.jpg'\n                image = cv2.imread(image_path)\n                w6[m].append(image)  \n                    \n        if (i==7):            \n            for k in df['image_id'].unique():\n               \n                image_path = single_word_path + 'mouth'+ str(k) + '.jpg'\n                image = cv2.imread(image_path)\n                w7[m].append(image)       \n        if (i==8):            \n            for k in df['image_id'].unique():\n                \n                image_path = single_word_path + 'mouth'+ str(k) + '.jpg'\n                image = cv2.imread(image_path)\n                w8[m].append(image)\n                    \n        if (i==9):            \n            for k in df['image_id'].unique():\n                \n                image_path = single_word_path + 'mouth'+ str(k) + '.jpg'\n                image = cv2.imread(image_path)\n                w9[m].append(image)   \n                    \n        if (i==10):            \n            for k in df['image_id'].unique():\n                \n                image_path = single_word_path + 'mouth'+ str(k) + '.jpg'\n                image = cv2.imread(image_path)\n                w10[m].append(image)               \n                    \n    m = m+1                ","metadata":{"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"b","metadata":{"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"[]"},"metadata":{}}]},{"cell_type":"code","source":"b = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n     1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n     2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,\n     3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,\n     4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,\n     5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,\n     6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,\n     7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,\n     8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,\n     9,9,9,9,9,9,9,9,9,9,9,9,9,9,9]","metadata":{"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"len(b)","metadata":{"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"150"},"metadata":{}}]},{"cell_type":"code","source":"a = w1 + w2 + w3 + w4 + w5 + w6 + w7 + w8 + w9 + w10","metadata":{"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"len(a)","metadata":{"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"150"},"metadata":{}}]},{"cell_type":"code","source":"c = a.copy()","metadata":{"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"Train_data = (np.array(a)/255.0).astype(np.float16)\nTrain_label = np.eye(10)[np.array(b).reshape(-1)]","metadata":{"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"Train_data.shape","metadata":{"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"(150, 10, 128, 128, 3)"},"metadata":{}}]},{"cell_type":"code","source":"Train_label.shape","metadata":{"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"(150, 10)"},"metadata":{}}]},{"cell_type":"code","source":"x = np.arange(Train_label.shape[0])\nnp.random.shuffle(x)\n# same order shuffle is needed\nTrain_label = Train_label[x]\nTrain_data = Train_data[x]\n\n# declare data for training and validation, if you want, you can seperate testset from this\nX_train = Train_data[0:130,:]\nY_train = Train_label[0:130]\nX_test = Train_data[130:,:]\nY_test = Train_label[130:]","metadata":{"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport keras\nimport os # drectory library\nfrom keras.layers import Activation\nfrom keras.layers import Dropout\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Input\nfrom keras.layers import Flatten\nfrom keras.layers import Input\nfrom keras.layers.wrappers import TimeDistributed\nfrom keras import applications\nfrom keras import optimizers\nfrom keras.models import Model\nfrom keras.models import load_model","metadata":{"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"timesteps = 10 # input frame numbers for LSTM\nn_labels = 10 # Number of Dataset Labels\nLearning_rate = 0.00001 # Oprimizers lr, in this case, for adam\nbatch_size = 8\nvalidation_ratio = 0.1\n#num_epochs = 2\nimg_col = 128 # Transfer model input size ( MobileNet )\nimg_row = 128 # Transfer model input size ( MobileNet )\nimg_channel = 3 # RGB","metadata":{"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"!pip install git+https://github.com/rcmalli/keras-vggface.git\n!pip install keras_vggface","metadata":{"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/rcmalli/keras-vggface.git\n  Cloning https://github.com/rcmalli/keras-vggface.git to /tmp/pip-req-build-xr0j47cs\n  Running command git clone -q https://github.com/rcmalli/keras-vggface.git /tmp/pip-req-build-xr0j47cs\nRequirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.7/site-packages (from keras-vggface==0.6) (1.19.5)\nRequirement already satisfied: scipy>=0.14 in /opt/conda/lib/python3.7/site-packages (from keras-vggface==0.6) (1.5.4)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras-vggface==0.6) (2.10.0)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (from keras-vggface==0.6) (7.2.0)\nRequirement already satisfied: keras in /opt/conda/lib/python3.7/site-packages (from keras-vggface==0.6) (2.4.3)\nRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from keras-vggface==0.6) (1.15.0)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from keras-vggface==0.6) (5.3.1)\nBuilding wheels for collected packages: keras-vggface\n  Building wheel for keras-vggface (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for keras-vggface: filename=keras_vggface-0.6-py3-none-any.whl size=8310 sha256=6e3c7984f2469b497eefcab040bd84057074d5c42cd8fecb9b8afdc2a6c1bf78\n  Stored in directory: /tmp/pip-ephem-wheel-cache-xod7obhm/wheels/08/df/86/0225d44647ab2256dbf1e006823288fe9cc86367a056e6ea2c\nSuccessfully built keras-vggface\nInstalling collected packages: keras-vggface\nSuccessfully installed keras-vggface-0.6\nRequirement already satisfied: keras_vggface in /opt/conda/lib/python3.7/site-packages (0.6)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (from keras_vggface) (7.2.0)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from keras_vggface) (5.3.1)\nRequirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.7/site-packages (from keras_vggface) (1.19.5)\nRequirement already satisfied: scipy>=0.14 in /opt/conda/lib/python3.7/site-packages (from keras_vggface) (1.5.4)\nRequirement already satisfied: keras in /opt/conda/lib/python3.7/site-packages (from keras_vggface) (2.4.3)\nRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from keras_vggface) (1.15.0)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras_vggface) (2.10.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install Keras-Applications","metadata":{"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Collecting Keras-Applications\n  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n\u001b[K     |████████████████████████████████| 50 kB 731 kB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.7/site-packages (from Keras-Applications) (1.19.5)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from Keras-Applications) (2.10.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from h5py->Keras-Applications) (1.15.0)\nInstalling collected packages: Keras-Applications\nSuccessfully installed Keras-Applications-1.0.8\n","output_type":"stream"}]},{"cell_type":"code","source":"import keras\nfrom keras import applications","metadata":{"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"from keras_vggface.vggface import VGGFace\n\n# Based on VGG16 architecture -> old paper(2015)\nmodel = VGGFace(input_shape=(img_col,img_row,img_channel),include_top=False, model='vgg16')\n#resnet50","metadata":{"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Downloading data from https://github.com/rcmalli/keras-vggface/releases/download/v2.0/rcmalli_vggface_tf_notop_vgg16.h5\n58916864/58909280 [==============================] - 1s 0us/step\n","output_type":"stream"}]},{"cell_type":"code","source":"\n#model = applications.VGG16(input_shape=(img_col,img_row,img_channel), weights=\"imagenet\", include_top=False)\nmodel.trainable = False\n# FC Dense Layer\nx = model.output\nx = Flatten()(x)\nx = Dense(1024, activation=\"relu\")(x)\nx = Dropout(0.3)(x)\ncnn_out = Dense(128, activation=\"relu\")(x)\n# Construct CNN model \nLstm_inp = Model(model.input, cnn_out)\n\nvideo = Input(shape=(timesteps,img_col,img_row,img_channel))\n\n# Distribute CNN output by timesteps \nencoded_frames = TimeDistributed(Lstm_inp)(video)\n# Contruct LSTM model \nencoded_sequence = LSTM(256)(encoded_frames)\nhidden_Drop = Dropout(0.3)(encoded_sequence)\nhidden_layer = Dense(128, activation=\"relu\")(encoded_sequence)\noutputs = Dense(n_labels, activation=\"softmax\")(hidden_layer)\n# Contruct CNN+LSTM model \nmodel = Model([video], outputs)\n# 3. Setting up the Model Learning Process\n# Model Compile \nadam = keras.optimizers.Adam(lr=Learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=adam, metrics=[\"accuracy\"])\n\n","metadata":{"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# 4. Training the Model\nhist = model.fit(X_train, Y_train, batch_size=batch_size, validation_split=validation_ratio, shuffle=True, epochs=100, verbose = 2)","metadata":{"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Epoch 1/100\n15/15 - 74s - loss: 2.3027 - accuracy: 0.0940 - val_loss: 2.3027 - val_accuracy: 0.0769\nEpoch 2/100\n15/15 - 46s - loss: 2.3013 - accuracy: 0.1197 - val_loss: 2.3031 - val_accuracy: 0.0769\nEpoch 3/100\n15/15 - 46s - loss: 2.3002 - accuracy: 0.1709 - val_loss: 2.3031 - val_accuracy: 0.0000e+00\nEpoch 4/100\n15/15 - 46s - loss: 2.2991 - accuracy: 0.1453 - val_loss: 2.3035 - val_accuracy: 0.0000e+00\nEpoch 5/100\n15/15 - 46s - loss: 2.2986 - accuracy: 0.1453 - val_loss: 2.3034 - val_accuracy: 0.0000e+00\nEpoch 6/100\n15/15 - 46s - loss: 2.2970 - accuracy: 0.1453 - val_loss: 2.3043 - val_accuracy: 0.0000e+00\nEpoch 7/100\n15/15 - 46s - loss: 2.2957 - accuracy: 0.1624 - val_loss: 2.3041 - val_accuracy: 0.0000e+00\nEpoch 8/100\n15/15 - 46s - loss: 2.2948 - accuracy: 0.1538 - val_loss: 2.3038 - val_accuracy: 0.0000e+00\nEpoch 9/100\n15/15 - 46s - loss: 2.2929 - accuracy: 0.1538 - val_loss: 2.3043 - val_accuracy: 0.0000e+00\nEpoch 10/100\n15/15 - 46s - loss: 2.2916 - accuracy: 0.1538 - val_loss: 2.3039 - val_accuracy: 0.0769\nEpoch 11/100\n15/15 - 46s - loss: 2.2893 - accuracy: 0.1795 - val_loss: 2.3044 - val_accuracy: 0.0769\nEpoch 12/100\n15/15 - 46s - loss: 2.2870 - accuracy: 0.1795 - val_loss: 2.3044 - val_accuracy: 0.0769\nEpoch 13/100\n15/15 - 46s - loss: 2.2844 - accuracy: 0.1880 - val_loss: 2.3039 - val_accuracy: 0.0769\nEpoch 14/100\n15/15 - 46s - loss: 2.2823 - accuracy: 0.2051 - val_loss: 2.3026 - val_accuracy: 0.0000e+00\nEpoch 15/100\n15/15 - 46s - loss: 2.2785 - accuracy: 0.2051 - val_loss: 2.3037 - val_accuracy: 0.0769\nEpoch 16/100\n15/15 - 46s - loss: 2.2766 - accuracy: 0.1880 - val_loss: 2.3042 - val_accuracy: 0.0769\nEpoch 17/100\n15/15 - 46s - loss: 2.2729 - accuracy: 0.1795 - val_loss: 2.3033 - val_accuracy: 0.0769\nEpoch 18/100\n15/15 - 46s - loss: 2.2684 - accuracy: 0.1966 - val_loss: 2.3025 - val_accuracy: 0.0769\nEpoch 19/100\n15/15 - 46s - loss: 2.2647 - accuracy: 0.1880 - val_loss: 2.3031 - val_accuracy: 0.0769\nEpoch 20/100\n15/15 - 46s - loss: 2.2599 - accuracy: 0.1966 - val_loss: 2.3032 - val_accuracy: 0.0769\nEpoch 21/100\n15/15 - 46s - loss: 2.2551 - accuracy: 0.1880 - val_loss: 2.3040 - val_accuracy: 0.0769\nEpoch 22/100\n15/15 - 46s - loss: 2.2485 - accuracy: 0.2051 - val_loss: 2.3018 - val_accuracy: 0.0769\nEpoch 23/100\n15/15 - 46s - loss: 2.2425 - accuracy: 0.2051 - val_loss: 2.3015 - val_accuracy: 0.0769\nEpoch 24/100\n15/15 - 46s - loss: 2.2346 - accuracy: 0.1880 - val_loss: 2.3028 - val_accuracy: 0.0769\nEpoch 25/100\n15/15 - 46s - loss: 2.2269 - accuracy: 0.2051 - val_loss: 2.3007 - val_accuracy: 0.0769\nEpoch 26/100\n15/15 - 46s - loss: 2.2164 - accuracy: 0.2308 - val_loss: 2.3003 - val_accuracy: 0.0000e+00\nEpoch 27/100\n15/15 - 46s - loss: 2.2094 - accuracy: 0.2906 - val_loss: 2.2940 - val_accuracy: 0.0000e+00\nEpoch 28/100\n15/15 - 46s - loss: 2.1975 - accuracy: 0.2650 - val_loss: 2.2965 - val_accuracy: 0.0000e+00\nEpoch 29/100\n15/15 - 46s - loss: 2.1819 - accuracy: 0.2650 - val_loss: 2.2905 - val_accuracy: 0.0000e+00\nEpoch 30/100\n15/15 - 46s - loss: 2.1698 - accuracy: 0.2906 - val_loss: 2.2885 - val_accuracy: 0.0000e+00\nEpoch 31/100\n15/15 - 46s - loss: 2.1551 - accuracy: 0.3162 - val_loss: 2.2780 - val_accuracy: 0.0000e+00\nEpoch 32/100\n15/15 - 46s - loss: 2.1363 - accuracy: 0.3162 - val_loss: 2.2760 - val_accuracy: 0.0000e+00\nEpoch 33/100\n15/15 - 46s - loss: 2.1159 - accuracy: 0.3504 - val_loss: 2.2664 - val_accuracy: 0.0769\nEpoch 34/100\n15/15 - 46s - loss: 2.0977 - accuracy: 0.3504 - val_loss: 2.2636 - val_accuracy: 0.0769\nEpoch 35/100\n15/15 - 46s - loss: 2.0670 - accuracy: 0.3333 - val_loss: 2.2509 - val_accuracy: 0.0769\nEpoch 36/100\n15/15 - 46s - loss: 2.0411 - accuracy: 0.3419 - val_loss: 2.2365 - val_accuracy: 0.0769\nEpoch 37/100\n15/15 - 46s - loss: 2.0089 - accuracy: 0.3077 - val_loss: 2.2135 - val_accuracy: 0.0769\nEpoch 38/100\n15/15 - 46s - loss: 1.9783 - accuracy: 0.2991 - val_loss: 2.2134 - val_accuracy: 0.0769\nEpoch 39/100\n15/15 - 46s - loss: 1.9428 - accuracy: 0.2991 - val_loss: 2.1714 - val_accuracy: 0.0769\nEpoch 40/100\n15/15 - 46s - loss: 1.9088 - accuracy: 0.3504 - val_loss: 2.1548 - val_accuracy: 0.0769\nEpoch 41/100\n15/15 - 46s - loss: 1.8708 - accuracy: 0.3248 - val_loss: 2.1238 - val_accuracy: 0.0000e+00\nEpoch 42/100\n15/15 - 46s - loss: 1.8321 - accuracy: 0.3419 - val_loss: 2.1172 - val_accuracy: 0.0769\nEpoch 43/100\n15/15 - 46s - loss: 1.8003 - accuracy: 0.3675 - val_loss: 2.0807 - val_accuracy: 0.0000e+00\nEpoch 44/100\n15/15 - 46s - loss: 1.7615 - accuracy: 0.3761 - val_loss: 2.0532 - val_accuracy: 0.0000e+00\nEpoch 45/100\n15/15 - 46s - loss: 1.7275 - accuracy: 0.4274 - val_loss: 2.0409 - val_accuracy: 0.0000e+00\nEpoch 46/100\n15/15 - 46s - loss: 1.6945 - accuracy: 0.4188 - val_loss: 2.0167 - val_accuracy: 0.0000e+00\nEpoch 47/100\n15/15 - 46s - loss: 1.6537 - accuracy: 0.4359 - val_loss: 1.9862 - val_accuracy: 0.0769\nEpoch 48/100\n15/15 - 46s - loss: 1.6336 - accuracy: 0.4188 - val_loss: 1.9412 - val_accuracy: 0.0000e+00\nEpoch 49/100\n15/15 - 46s - loss: 1.5876 - accuracy: 0.4530 - val_loss: 1.9222 - val_accuracy: 0.1538\nEpoch 50/100\n15/15 - 46s - loss: 1.5648 - accuracy: 0.4615 - val_loss: 1.9035 - val_accuracy: 0.0000e+00\nEpoch 51/100\n15/15 - 46s - loss: 1.5340 - accuracy: 0.4872 - val_loss: 1.8809 - val_accuracy: 0.0769\nEpoch 52/100\n15/15 - 46s - loss: 1.5080 - accuracy: 0.4701 - val_loss: 1.8669 - val_accuracy: 0.0000e+00\nEpoch 53/100\n15/15 - 46s - loss: 1.4756 - accuracy: 0.4872 - val_loss: 1.8393 - val_accuracy: 0.0000e+00\nEpoch 54/100\n15/15 - 46s - loss: 1.4488 - accuracy: 0.4786 - val_loss: 1.8200 - val_accuracy: 0.0769\nEpoch 55/100\n15/15 - 46s - loss: 1.4159 - accuracy: 0.4872 - val_loss: 1.8037 - val_accuracy: 0.0769\nEpoch 56/100\n15/15 - 46s - loss: 1.3980 - accuracy: 0.4872 - val_loss: 1.8008 - val_accuracy: 0.0000e+00\nEpoch 57/100\n15/15 - 46s - loss: 1.3780 - accuracy: 0.5214 - val_loss: 1.7827 - val_accuracy: 0.0000e+00\nEpoch 58/100\n15/15 - 46s - loss: 1.3480 - accuracy: 0.5556 - val_loss: 1.7297 - val_accuracy: 0.0769\nEpoch 59/100\n15/15 - 46s - loss: 1.3157 - accuracy: 0.5726 - val_loss: 1.7223 - val_accuracy: 0.0769\nEpoch 60/100\n15/15 - 46s - loss: 1.2887 - accuracy: 0.5897 - val_loss: 1.6908 - val_accuracy: 0.0000e+00\nEpoch 61/100\n15/15 - 46s - loss: 1.2858 - accuracy: 0.5726 - val_loss: 1.7074 - val_accuracy: 0.0769\nEpoch 62/100\n15/15 - 46s - loss: 1.2404 - accuracy: 0.6325 - val_loss: 1.7179 - val_accuracy: 0.0769\nEpoch 63/100\n15/15 - 46s - loss: 1.2428 - accuracy: 0.6154 - val_loss: 1.6845 - val_accuracy: 0.0000e+00\nEpoch 64/100\n15/15 - 46s - loss: 1.2133 - accuracy: 0.6325 - val_loss: 1.6456 - val_accuracy: 0.0769\nEpoch 65/100\n15/15 - 46s - loss: 1.2001 - accuracy: 0.6154 - val_loss: 1.6370 - val_accuracy: 0.0769\nEpoch 66/100\n15/15 - 46s - loss: 1.1864 - accuracy: 0.6410 - val_loss: 1.6400 - val_accuracy: 0.1538\nEpoch 67/100\n15/15 - 46s - loss: 1.1762 - accuracy: 0.6752 - val_loss: 1.6032 - val_accuracy: 0.0769\nEpoch 68/100\n15/15 - 46s - loss: 1.1440 - accuracy: 0.6325 - val_loss: 1.6027 - val_accuracy: 0.1538\nEpoch 69/100\n15/15 - 46s - loss: 1.1267 - accuracy: 0.7436 - val_loss: 1.6042 - val_accuracy: 0.1538\nEpoch 70/100\n15/15 - 46s - loss: 1.1122 - accuracy: 0.7009 - val_loss: 1.5891 - val_accuracy: 0.0769\nEpoch 71/100\n15/15 - 46s - loss: 1.0951 - accuracy: 0.7265 - val_loss: 1.5903 - val_accuracy: 0.1538\nEpoch 72/100\n15/15 - 46s - loss: 1.0805 - accuracy: 0.7179 - val_loss: 1.5735 - val_accuracy: 0.1538\nEpoch 73/100\n15/15 - 46s - loss: 1.0761 - accuracy: 0.7607 - val_loss: 1.5635 - val_accuracy: 0.1538\nEpoch 74/100\n15/15 - 46s - loss: 1.0348 - accuracy: 0.7778 - val_loss: 1.5402 - val_accuracy: 0.1538\nEpoch 75/100\n15/15 - 46s - loss: 1.0459 - accuracy: 0.7265 - val_loss: 1.5757 - val_accuracy: 0.2308\nEpoch 76/100\n15/15 - 46s - loss: 1.0173 - accuracy: 0.7607 - val_loss: 1.5086 - val_accuracy: 0.2308\nEpoch 77/100\n15/15 - 46s - loss: 1.0051 - accuracy: 0.7863 - val_loss: 1.5675 - val_accuracy: 0.2308\nEpoch 78/100\n15/15 - 46s - loss: 0.9894 - accuracy: 0.7521 - val_loss: 1.5054 - val_accuracy: 0.1538\nEpoch 79/100\n15/15 - 46s - loss: 0.9527 - accuracy: 0.8120 - val_loss: 1.4973 - val_accuracy: 0.3077\nEpoch 80/100\n15/15 - 46s - loss: 0.9336 - accuracy: 0.8547 - val_loss: 1.4808 - val_accuracy: 0.1538\nEpoch 81/100\n15/15 - 46s - loss: 0.9340 - accuracy: 0.7863 - val_loss: 1.4799 - val_accuracy: 0.1538\nEpoch 82/100\n15/15 - 46s - loss: 0.9034 - accuracy: 0.8120 - val_loss: 1.4797 - val_accuracy: 0.1538\nEpoch 83/100\n15/15 - 46s - loss: 0.9128 - accuracy: 0.8291 - val_loss: 1.4519 - val_accuracy: 0.1538\nEpoch 84/100\n15/15 - 46s - loss: 0.8765 - accuracy: 0.8547 - val_loss: 1.4541 - val_accuracy: 0.2308\nEpoch 85/100\n15/15 - 46s - loss: 0.8673 - accuracy: 0.8205 - val_loss: 1.4330 - val_accuracy: 0.1538\nEpoch 86/100\n15/15 - 46s - loss: 0.8544 - accuracy: 0.8376 - val_loss: 1.4442 - val_accuracy: 0.2308\nEpoch 87/100\n15/15 - 46s - loss: 0.8558 - accuracy: 0.7607 - val_loss: 1.4825 - val_accuracy: 0.1538\nEpoch 88/100\n15/15 - 46s - loss: 0.8481 - accuracy: 0.8120 - val_loss: 1.3889 - val_accuracy: 0.2308\nEpoch 89/100\n15/15 - 46s - loss: 0.8404 - accuracy: 0.8120 - val_loss: 1.4164 - val_accuracy: 0.2308\nEpoch 90/100\n15/15 - 46s - loss: 0.8053 - accuracy: 0.8547 - val_loss: 1.4291 - val_accuracy: 0.1538\nEpoch 91/100\n15/15 - 46s - loss: 0.8019 - accuracy: 0.8376 - val_loss: 1.4064 - val_accuracy: 0.3077\nEpoch 92/100\n15/15 - 46s - loss: 0.7742 - accuracy: 0.8376 - val_loss: 1.3909 - val_accuracy: 0.2308\nEpoch 93/100\n15/15 - 46s - loss: 0.7696 - accuracy: 0.8803 - val_loss: 1.3740 - val_accuracy: 0.2308\nEpoch 94/100\n15/15 - 46s - loss: 0.7603 - accuracy: 0.8547 - val_loss: 1.4101 - val_accuracy: 0.2308\nEpoch 95/100\n15/15 - 46s - loss: 0.7679 - accuracy: 0.8632 - val_loss: 1.3702 - val_accuracy: 0.1538\nEpoch 96/100\n15/15 - 46s - loss: 0.7446 - accuracy: 0.8718 - val_loss: 1.4089 - val_accuracy: 0.3077\nEpoch 97/100\n15/15 - 46s - loss: 0.7470 - accuracy: 0.8718 - val_loss: 1.3456 - val_accuracy: 0.1538\nEpoch 98/100\n15/15 - 46s - loss: 0.7097 - accuracy: 0.8803 - val_loss: 1.3632 - val_accuracy: 0.1538\nEpoch 99/100\n15/15 - 46s - loss: 0.6895 - accuracy: 0.8803 - val_loss: 1.3670 - val_accuracy: 0.3077\nEpoch 100/100\n15/15 - 46s - loss: 0.6744 - accuracy: 0.9145 - val_loss: 1.3383 - val_accuracy: 0.3077\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(X_test)","metadata":{"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"pred_array = []","metadata":{"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"for i in range(len(Y_test)):\n    pred_array.append(np.argmax(y_pred[i]))","metadata":{"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"pred_array","metadata":{"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"[3, 3, 5, 1, 2, 9, 1, 5, 3, 3, 3, 3, 3, 6, 8, 1, 4, 4, 5, 2]"},"metadata":{}}]},{"cell_type":"code","source":"y_t=[]","metadata":{"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"for i in range(len(Y_test)):\n    y_t.append(np.argmax(Y_test[i]))","metadata":{"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\naccuracy_score(pred_array, y_t)","metadata":{"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"0.2"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.metrics import f1_score","metadata":{"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"f1_score(pred_array, y_t, average='macro')","metadata":{"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"0.12666666666666665"},"metadata":{}}]},{"cell_type":"code","source":"f1_score(pred_array, y_t, average='micro')","metadata":{"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"0.20000000000000004"},"metadata":{}}]},{"cell_type":"code","source":"model.save('Lib_Reading_10Frame_Model.h5')","metadata":{"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}